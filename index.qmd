---
title: "Let's talk about Thurstone & Co.: An information-theoretical model for comparative judgments, and its statistical translation"
author:
  - name: 
      given: Jose Manuel
      family: Rivera Espejo
    orcid: 0000-0002-3088-2783
    url: https://www.uantwerpen.be/en/staff/jose-manuel-rivera-espejo_23166/
    email: JoseManuel.RiveraEspejo@uantwerpen.be
    corresponding: true
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Tine
      family: van Daal
      non-dropping-particle: van
    orcid: https://orcid.org/0000-0001-9398-9775
    url: https://www.uantwerpen.be/en/staff/tine-vandaal/
    email: tine.vandaal@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Sven
      family: De Maeyer
      non-dropping-particle: De
    orcid: 0000-0003-2888-1631
    url: https://www.uantwerpen.be/en/staff/sven-demaeyer/
    email: sven.demaeyer@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Steven
      family: Gillis
    orcid: 
    url: https://www.uantwerpen.be/nl/personeel/steven-gillis/
    email: steven.gillis@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Linguistics
        group: Centre for computational linguistics, psycholinguistics, and sociolinguistics (CLiPS)
funding: 
  statement: "The project was founded through the Research Fund of the University of Antwerp (BOF)."
keywords:
  - Probability
  - Directed Acyclic Graphs
  - Bayesian methods
  - Thurstonian model
  - Comparative judgement
  - Structural Causal Models
  - Statistical modeling
abstract: |
  (to do)
key-points:
  - (to do)
date: last-modified
bibliography: references.bib
notebook-links: global
lightbox: true
citation: true
  # type: article-journal
  # container-title: "Psychometrika"
  # doi: ""
  # # url: https://example.com/summarizing-output
execute: 
  cache: true
  # eval: true
  echo: false
  # output: true
  # include: true
  warning: false
  error: false
  message: false
---

<!-- abstract -->
<!-- recording: Sven and Tine 24.11.25; time: 00:58:20 - 00:59:20 -->


# Introduction {#sec-introduction}

<!-- recording: Sven 24.10.04; time: 00:00:00 - 00:09:10 -->
<!-- recording: Sven 24.10.04; time: 00:14:30 - 00:21:00 -->
<!-- recording: Sven and Tine 24.10.18; time: 00:04:05 - 00:08:30 -->
<!-- recording: Sven and Tine 24.10.25; time: 00:00:00 - 00:17:00 -->

<!-- 1. Overview of CJ method in assessing traits -->
In *comparative judgment* (CJ) studies, judges assess a specific trait or attribute across various stimuli by performing pairwise comparisons [@Thurstone_1927a; @Thurstone_1927b]. Each comparison produces a dichotomous outcome, indicating which stimulus is perceived to exhibit a higher trait level. For example, when assessing text quality, judges compare pairs of written texts (the stimuli) to determine the relative quality each text exhibit (the trait) [@Laming_2004; @Pollitt_2012b; @Whitehouse_2012; @vanDaal_et_al_2016; @Lesterhuis_2018_thesis; @Coertjens_et_al_2017; @Goossens_et_al_2018; @Bouwer_et_al_2023].

<!-- and spoken language [@Boonen_et_al_2020]. Additionally, it has been applied to evaluate various competencies, including mathematical problem-solving skills [@Jones_et_al_2015], engineering design skills [@Bartholomew_et_al_2018], build real-time web-based portfolios of performance [@@Kimbell_2012], conceptual understanding in algebra [@Jones_et_al_2019], statistical and English knowledge [@Marshall_et_al_2020], and STEM knowledge and skills [@Bartholomew_et_al_2020]. -->

<!-- 2. What is the effectiveness of CJ -->
Numerous studies have documented the effectiveness of CJ in assessing traits and competencies over the past decade. These studies have emphasized three aspects of the method's effectiveness: its reliability, validity, and practical applicability. Research on reliability indicates that CJ requires a relatively small number of pairwise comparisons [@Verhavert_et_al_2019; @Crompvoets_et_al_2022] to produce trait scores that are as precise and consistent as those generated by other assessment methods [@Coertjens_et_al_2017; @Goossens_et_al_2018; @Bouwer_et_al_2023]. Furthermore, evidence suggests that the reliability and time efficiency of CJ are comparable, if not superior, to those of other assessment methods when employing adaptive comparison algorithms [@Pollitt_2012b; @Verhavert_et_al_2022; @Mikhailiuk_et_al_2021]. Meanwhile, research on validity suggests that scores generated by CJ can accurately represent the traits under measurement [@Whitehouse_2012; @vanDaal_et_al_2016; @Lesterhuis_2018_thesis; @Bartholomew_et_al_2018; @Bouwer_et_al_2023], while research on practical applicability highlights the method's versatility across both educational and non-educational contexts [@Kimbell_2012; @Jones_et_al_2015; @Bartholomew_et_al_2018; @Jones_et_al_2019; @Marshall_et_al_2020; @Bartholomew_et_al_2020; @Boonen_et_al_2020].

<!-- rubrics [@Coertjens_et_al_2017; @Goossens_et_al_2018]  -->
<!-- holistic benchmark ratings [@Bouwer_et_al_2023] -->

<!-- Adaptive comparison algorithms dynamically present stimuli to judges, based on the results of previous comparisons. They aim to enhance the informativeness of comparisons while optimizing judges' time [@Bramley_2015].  -->

<!-- 3. But what critical issues remain in CJ Research?, what is the organization of the study? -->
Nevertheless, despite the increasing number of CJ studies, unsystematic and fragmented research approaches have left several critical issues unaddressed. The present study primarily focuses on two: the overreliance on the assumptions of Thurstone's Case V in the statistical analysis of CJ data, and the apparent disconnect between CJ's approach to trait measurement and hypothesis testing. The following sections begin with a brief overview of Thurstone's theory and a detailed examination of these issues. Subsequently, the study introduces a theoretical model for CJ that builds upon Thurstone's theory, alongside its statistical translation, designed to address the two concerns simultaneously.  


# Thurstone's theory {#sec-thurstone_theory}
<!-- recording: Sven and Tine 24.11.18; time: 00:04:30 - 00:14:50 -->
<!-- recording: Sven and Tine 24.11.18; time: 00:19:04 - 00:21:37 -->
<!-- recording: Sven and Tine 24.11.25; time: 00:00:35 - 00:06:55 -->
<!-- recording: Sven and Tine 24.11.25; time: 00:30:35 - 00:35:10 -->

<!-- 1. Thurstone's Theory of CJ: discriminal process -->
In its most general form, Thurstone’s theory addresses pairwise comparisons of stimuli made by a single judge [@Thurstone_1927b, pp. 267]. The theory posits that two key factors determine the dichotomous outcome of these comparisons: the discriminal process of each stimulus and their discriminal difference. The *discriminal process* represents the psychological impact each stimulus has on judges or, more simply, their perception of the stimulus trait. According to the theory, the discriminal process for each stimulus follows a Normal distribution, where its mode (mean), called the *modal discriminal process*, indicates the stimulus position on the trait continuum, while its dispersion, known as the *discriminal dispersion*, reflects variability in the perceived trait of the stimulus.

<!-- 2. Example of discriminal processes -->
@fig-discriminal_process illustrates the discriminal processes along a quality trait continuum for two written texts. The figure shows that each discriminal process follows a Normal distribution. It also indicates that the modal discriminal process for Text B is positioned further along the continuum than that of Text A $(S_{B} > S_{A})$, signifying a higher quality for Text B. Additionally, the figure highlights that Text B has a broader distribution compared to Text A, which arises from its larger discriminal dispersion $(\sigma_{B} > \sigma_{A})$.

<!-- 1. Thurstone's Theory of CJ: discriminal process -->
<!-- In its most general form, Thurstone’s theory addresses pairwise comparisons of stimuli made by a single judge [@Thurstone_1927b, pp. 267]. The theory posits that two key factors determine the dichotomous outcome of these comparisons: the discriminal process of each stimulus and their discriminal difference. The *discriminal process* represents the psychological impact each stimulus has on judges or, more simply, their perception of the stimulus trait. For instance, according to the theory, the discriminal process for a text along a quality trait continuum follows a Normal distribution, as depicted in @fig-discriminal_process. In this distribution, the mode or mean, termed the *modal discriminal process*, represents the stimulus's position on the trait continuum. @fig-discriminal_process show the modal discriminal process for Text B is positioned further along the continuum than that of Text A $(S_{B} > S_{A})$. Additionally, the dispersion of this distribution, called the *discriminal dispersion*, captures variability in the perceived trait of the stimulus. @fig-discriminal_process highlights that Text B has a broader distribution compared to Text A, which arises from its larger discriminal dispersion $(\sigma_{B} > \sigma_{A})$. -->

![Discriminal processes for two written texts](/images/figures/discriminal_process.png){#fig-discriminal_process width=70%}

<!-- 3. Thurstone's Theory of CJ: discriminal difference -->
However, since the discriminal processes of the stimuli are not directly observable, the theory introduces the *law of comparative judgment*. This law states that in pairwise comparisons, a judge perceives the stimulus positioned further along the trait continuum as having a higher level of that trait. This law emphasizes that the relative distance between stimuli, rather than their absolute positions on the continuum, determines the outcome of the pairwise comparison. Indeed, the theory assumes that the observed dichotomous outcome arises from the difference between the underlying discriminal processes of the stimuli, referred to as the *discriminal difference*. Since the individual discriminal processes follow a Normal distribution, the discriminal difference also follows a Normal distribution [@Andrich_1978]. In this distribution, the mode (mean) represents the relative separation between the stimuli and the dispersion captures the variability of that relative separation.

<!-- The mode (mean) of this distribution, representing the (average) relative separation, is given by the difference between the modal discriminal processes of the stimuli $S_{BA}=S_{B}-S_{A}$. Meanwhile, the dispersion of the distribution, reflecting the variability in the relative separation, is calculated as $\sigma_{BA} = \sqrt{\sigma_{B}^{2} + \sigma_{A}^{2} - \rho\sigma_{B}\sigma_{A}}$. Here, $\sigma_{B}$ and $\sigma_{A}$ denote the discriminal dispersions of the stimuli, while $\rho$ represents the correlation between their discriminal processes. This correlation quantifies the dependence of the judge's perception of the trait in one stimulus on his perception of the same trait in another. -->

<!-- 4. Example of discriminal difference -->
@fig-discriminal_difference illustrates the distribution of the discriminal difference for the texts presented in @fig-discriminal_process. The figure indicates that the judge perceives Text B as having significantly higher quality than Text A. This conclusion is evident from the positive difference in their modal discriminal processes $(S_{B} - S_{A} > 0)$ and the area under the curve where the discriminal difference distinctly favors Text B over Text A, denoted as $P(B > A)$ (shaded gray area). As a result, the dichotomous outcome of this comparison is more likely to favor Text B over Text A.

![Discriminal difference for two written texts](/images/figures/discriminal_difference.png){#fig-discriminal_difference width=70%}


# The two critical issues in CJ literature {#sec-theory-issues}

This section examines the two critical issues in the CJ literature that serve as the primary focus of this study. The first is the overreliance on Thurstone’s Case V assumptions in the statistical analysis of CJ data. The second is the apparent disconnect between CJ's approach to trait measurement and hypothesis testing.

## The Case V and the statistical analysis of CJ data {#sec-theory-issue1}

<!-- recording: Sven and Tine 24.10.25; time: 00:31:10 - 00:52:20 -->
<!-- recording: Sven and Tine 24.11.12; time: 00:00:00 - 00:00:00 -->
<!-- recording: Sven and Tine 24.11.18; time: 00:00:25 - 00:04:30 -->
<!-- recording: Sven and Tine 24.11.18; time: 00:14:50 - 00:19:04 -->
<!-- recording: Sven and Tine 24.11.18; time: 00:21:37 - 00:47:20 -->
<!-- recording: Sven and Tine 24.11.25; time: 00:06:55 - 00:24:50 -->
<!-- recording: Sven and Tine 24.11.25; time: 00:35:10 - 00:49:30 -->

<!-- 1. Thurstone's Cases and their simplifying assumptions  -->
Since the general form of Thurstone's theory, outlined in @sec-thurstone_theory, applies to a CJ design where a single judge evaluates multiple stimuli, Thurstone developed four additional cases for the theory's practical application, each progressively incorporating additional simplifying assumptions. Case I represents the general form of the theory. Case II extends this by allowing multiple judges to make comparisons rather than restricting the comparisons to a single judge. Case III introduces the assumption of zero correlation between stimuli. Case IV builds on this by assuming the stimuli have similar dispersions. Finally, Case V replaces this assumption with the condition that the stimuli have equal discriminal dispersions. @tbl-thurstone_cases summarizes these cases and their assumptions. For a detailed discussion of this progression, refer to @Thurstone_1927b and @Bramley_2008 [pp. 248-253].

<!-- , suggesting all stimuli have the same variability in their perceived trait levels -->

![Thurstones cases and their asumptions](/images/tables/thurstone_cases.png){#tbl-thurstone_cases width=90%}

<!-- 2. The Case V and its simpler statistical representation: the BTL model -->
Despite its reliance on the largest number of simplifying assumptions [@Bramley_2008, pp. 253; @Kelly_et_al_2022, pp. 677], Case V remains the most widely used case in the CJ literature. This popularity stems mainly from its simplified statistical representation in the Bradley-Terry-Luce (BTL) model [@Bradley_et_al_1952; @Luce_1959]. The BTL model mirrors the assumptions of Case V, with one key difference: while Case V assumes a Normal distribution for the stimuli's discriminal processes, the BTL model uses the more mathematically tractable Logistic distribution [@Andrich_1978; @Bramley_2008, pp. 254] (see @tbl-thurstone_cases). This substitution has little impact on the model's estimation or interpretation, as the Normal and Logistic distributions share similar statistical properties, differing only by a scaling factor of approximately $1.7$ [@vanderLinden_et_al_2017_I, pp. 16] (see @fig-logistic_vs_normal).

<!-- the mathematical function linking dichotomous outcomes to trait scores -->

<!-- Model misspecification happens when the set of probability distributions considered by the statistician does not include the distribution that generated the observed data. -->


::: {#fig-logistic_vs_normal layout-nrow=2}

![Probability density](/images/figures/density.png){#fig-density width=70%}

![Cummulative probability](/images/figures/cummulative.png){#fig-cumulative width=70%}

Probability density and cumulative probability of the logistic and Normal distributions. Extracted from @Bramley_2008 [pp. 254-255].
:::

<!-- 3. But what are limitations of Case V (and the BTL model)? -->
Nevertheless, Thurstone originally developed Case V to provide a "rather coarse scaling" of traits [@Thurstone_1927b, pp. 269], prioritizing statistical simplicity over precision in trait measurement [@Kelly_et_al_2022, pp. 677]. Thurstone explicitly warned against its untested application, stating that its use "should not be made without (an) experimental test" [@Thurstone_1927b, pp. 270], acknowledging that some assumptions could be problematic when researchers asesss complex traits or heterogeneous stimuli [@Thurstone_1927a, pp. 376]. Consequently, given that modern CJ applications frequently involve such traits and stimuli, two main assumptions of Case V and, by extension, of the BTL model may not consistently hold in theory or practice: the assumption of equal dispersion and zero correlation between stimuli.

<!-- Adding assumptions simplifies the formula and improves its tractability, but one cannot place as much confidence in the outputs of the formula if those assumptions are not met [@Kelly_et_al_2022, pp. 677]. -->


### The assumption of equal dispersions between stimuli {#sec-theory-issue1a}

<!-- 1. What does the equal dispersion between stimuli imply? -->
The discriminal dispersions of stimuli are crucial in determining the dichotomous outcomes of pairwise comparisons. Holding all other modeling factors constant, discrepancies in these dispersions shape the distribution of the discriminal difference, directly influencing the comparison outcome. @fig-dispersion illustrates how more uncertainty in the trait perception of one text relative to another, $(\sigma_{B}-\sigma_{A})$, broadens the distribution of their discriminal difference. This broadening affects the area under the curve where the discriminal difference distinctly favors one text over the other, $P(B > A)$. Additionally, the figure shows that when the discriminal dispersions of the texts are equal $(\sigma_{B}-\sigma_{A}=0)$, the discriminal difference is more likely to favor one text over the other than when their dispersions differ (shaded gray area).

![The discrepancy in the dispersions of stimuli and their effect on the distribution of the discriminal difference](/images/figures/dispersion.png){#fig-dispersion width=70%}

<!-- 2. But does it hold?  -->
However, Thurstone contended that this assumption of equal dispersions may not hold when researchers assess complex traits or heterogeneous stimuli [@Thurstone_1927a, pp. 376], as these traits and stimuli can introduce judgment discrepancies due to their unique features [@vanDaal_et_al_2016; @Lesterhuis_2018; @Chambers_et_al_2022]. Indeed, evidence of such violation may already exist in the CJ literature as misfit statistics. These statistics measure the judgment discrepancies associated with a given stimulus [@Pollitt_2004, pp. 12; @Goossens_et_al_2018, pp. 20]. For instance, labeling texts as "misfits" indicates that comparisons involving these texts result in more judgment discrepancies than others [@Pollitt_2012a; @Pollitt_2012b; @vanDaal_et_al_2016; @Goossens_et_al_2018]. This finding implies that discriminal differences associated with "misfits" texts usually display a broader dispersion, suggesting that the discriminal processes of these texts also exhibit more variation than other texts. Notably, this reasoning also applies to "misfit" judges, whose evaluations reflect substantial deviations from the shared consensus due to the unique characteristics of the stimuli or the judges themselves. Moreover, the presence of these "misfit" judges and their deviations can introduce additional statistical and measurement issues, which we discuss in @sec-theory-issue1b.

<!-- It is probably true that this variability of the discriminal dispersion on the psychological continuum is of relatively less serious importance in dealing with strictly homogeneous stimulus series, but it becomes a serious factor in dealing with less conspicuous attributes or with less homogeneous stimulus series, such as handwriting specimens, English compositions, sewing samples, oriental rugs. [@Thurstone_1927a, pp. 376] -->

<!-- Of these, the most questionable is the one that the discriminal dispersions of all the objects are equal. Thurstone clearly did not expect this to hold for any but the most simple stimuli. The scripts used in cross-moderation exercises are obviously far more complex than any used by Thurstone, so it seems naive to expect this assumption to hold here [@Bramley_2008]. -->

<!-- If a portfolio’s WMS exceeds the criterion of mean plus two standard deviations, this means the judges did not judge it consistently, some considering it ‘better’ than others did. [@Pollitt_2012a, pp. 165] -->

<!-- Representations with a large infit are representations which lead to more inconsistent judgments [@Goossens_et_al_2018, pp. 20] -->

<!-- a misfit statistic. Essentially this reports the amount of inconsistency in the various judgements that have been made of that script. It can act as a flag identifying scripts that judges find difficult to judge [@Pollitt_2004, pp. 12] -->

<!-- For instance, in evaluating creativity, a student might complete an assignment in a highly creative but unexpected manner. Judges, unprepared for such originality, may produce more divergent evaluations of the assignment. These increased discrepancies in judgments suggest that the discriminal difference associated with the assignment has a broader dispersion, indicating greater variation in the discriminal process for this assignment compared to others. -->

<!-- "weak" writers consistently produce texts of low quality in all aspects that determine the quality of a text, such as grammatical accuracy, organization, and language use. In such cases, judges typically agree on their inferior quality, leading to a clear disfavoring of these texts compared to others. This agreement ultimately imply narrow dispersions for the discriminal differences related to these texts and, by extension, narrow discriminal processes for them. However, as texts improve along the trait continuum, with certain aspects showing improvement and others reaming unaltered, these variations introduce uncertainty into the judgments. Now, since judges often rely on various intricate characteristics of the texts to form their judgments [@vanDaal_et_al_2016; @Lesterhuis_2018; @Chambers_et_al_2022], these changes lead to greater discrepancies in their evaluations. This divergence ultimately imply broader discriminal differences and, by extension, more varied discriminal processes for the texts. -->

<!-- So it seem that the extreme of the discriminal process distribution of the stimuli have higher reliability than the center? -->


<!-- 3. But what are the challenges when these assumptions are violated? -->
Therefore, assuming equal dispersions between stimuli, despite its violation, can cause Case V (and the BTL model) to inflate the reliability of the outcome, resulting in inaccurate conclusions about the comparison. Moreover, ignoring the disparity in dispersions can lead to the neglect of critical differences in the reliability of the trait across these stimuli, leading to erroneous conclusions about the trait's estimates [@McElreath_2020, pp. 370]. Additionally, if researchers acknowledge that misfit statistics help identify these critical differences in dispersion, the usual practice in the CJ literature of excluding stimuli based on these statistics [@Pollitt_2012b; @vanDaal_et_al_2017; @Goossens_et_al_2018] risks discarding valuable information and introducing bias into the trait's estimates [@Zimmerman_1994; @McElreath_2020, chap. 12]. The direction and magnitude of these biases are often unpredictable, as they depend on the specific stimuli researchers exclude from the analysis. 

<!-- Reliability is a necessary but not sufficient condition for validity. Reliability can exist without validity but validity cannot exist without reliability [@Perron_et_al_2015, pp. 2] -->

<!-- People have been employing a model with assumptions that they do not know if those assumptions apply for their cases. -->


### The assumption of zero correlation between stimuli {#sec-theory-issue1b}

<!-- 1. What does the zero correlation between stimuli imply? -->
Similar to the discriminal dispersions, the correlation between discriminal processes plays a crucial role in determining the dichotomous outcomes of pairwise comparisons. Holding all other modeling factors constant, this correlation shapes the distribution of the discriminal difference, directly influencing the comparison outcome. @fig-correlation illustrates how the dependence in trait perception between two texts narrows the distribution of their discriminal difference. This narrowing affects the area under the curve where the discriminal difference distinctly favors one text over the other, $P(B>A)$. Moreover, the figure shows that when two texts are independent or uncorrelated $(\rho=0)$, their discriminal difference is less likely to favor one text over the other than when the texts are highly correlated (shaded gray area).

<!-- "It is a safe assumption that when the stimulus series is very homogeneous with no distracting attributes, the correlation between discriminal deviations is low and possible even zero" [@Thurstone_1927b, pp. 268]. -->

![The correlation between stimuli and its effect on the distribution of the discriminal difference](/images/figures/correlation.png){#fig-correlation width=70%}

<!-- 2. But does it hold?  -->
Notably, Thurstone's Case V and the BTL model assume independent trait perceptions across stimuli. Thurstone attributed this independence to the cancellation of potential judges' biases, driven by two opposing and equally weighted effects occurring during the pairwise comparisons [@Thurstone_1927b, pp. 268]. @Andrich_1978 mathematically demonstrated this cancellation using the BTL model under the assumption of discriminal processes with additive biases. However, it is easy to imagine at least two scenarios where the zero correlation assumption almost certainly does not hold: when the pairwise comparison involves multidimensional, complex traits with heterogeneous stimuli and when an additional hierarchical structure is relevant to the stimuli. 

<!-- 3. The first scenario where the assumption does not hold -->
In the first scenario, the intricate aspects of multidimensional, complex traits may introduce dependencies between the stimuli due to certain judges' biases that resist cancellation. Research on text quality suggests that when judges evaluate these traits, they often rely on various intricate characteristics of the stimuli to form their judgments [@vanDaal_et_al_2016; @Lesterhuis_2018; @Chambers_et_al_2022]. These additional relevant characteristics, which are unlikely to be equally weighted or opposing, can unevenly influence judges' perceptions, creating biases in their judgments and, ultimately, introducing dependencies between stimuli [@vanderLinden_et_al_2017_II, pp. 346]. For example, this could occur when a judge assessing the argumentative quality of a text places more weight on its grammatical accuracy than other judges, ultimately favoring texts with fewer errors but weaker arguments. While direct evidence for this specific scenario is lacking, studies such as @Pollitt_et_al_2003 demonstrate the presence of such biases, supporting the idea that the factors influencing pairwise comparisons may not always cancel out.

<!-- If the test taker’s position on one latent trait is fixed, the assumption of local stochastic independence requires the association between the responses to the items to vanish. Suppose the assumption is violated and more than one dimension is necessary to describe the test takers’ position in the latent space. The association between the responses to the items given one proficiency parameter does then not vanish. [@vanderLinden_et_al_2017_II, pp. 346]. -->

<!-- research shows that, in the presence of these traits and stimuli, the accuracy of the judges' trait perception is influenced by the rank-order distance of the stimuli [@vanDaal_et_al_2017; @Gijsen_et_al_2021], a coarse measure of the relative positioning of the stimuli within the trait continuum. -->

<!-- 4. The second scenario where the assumption does not hold -->
In the second scenario, the shared context or inherent connections created by additional hierarchical structures may further introduce dependencies between stimuli, a statistical phenomenon commonly known as clustering [@Everitt_et_al_2010]. Although the CJ literature acknowledges the presence of such hierarchical structures, the statistical handling of this extra source of dependency between stimuli has been inadequate. For example, when CJ data includes multiple samples of stimuli from the same individuals, researchers often rely on (average) estimated BTL scores to conduct subsequent analyses and tests at the individual hierarchical level [@Bramley_et_al_2019; @Boonen_et_al_2020; @Bouwer_et_al_2023; @vanDaal_et_al_2017; @Jones_et_al_2019; @Gijsen_et_al_2021]. However, this approach can introduce additional statistical and measurement issues, which we discuss in @sec-theory-issue2.

<!-- 5. But what are the challenges when this assumption is violated? -->
In any case, the psychometric and statistical literature emphasizes the need to address factors that create dependencies between stimuli, as failing to do so can affect the reliability of the comparison outcomes and lead to inaccurate conclusions about the trait's estimates. For instance, researchers who overlook additional relevant traits, such as judges' biases, can cause dimensional mismatches in the statistical model used for analysis. This mismatch can artificially inflate the reliability of the trait [@Hoyle_et_al_2023, pp. 341] or, even worse, introduce bias into the trait's estimates [@Ackerman_1989]. Furthermore, as discussed in @sec-theory-issue1a, researchers who exclude judges based on misfit statistics can risk discarding valuable information, further biasing the trait's estimates [@Zimmerman_1994; @McElreath_2020, chap. 12]. Lastly, researchers who fail to account for hierarchical (grouping) structures can reduce the precision of model parameter estimates, which may amplify the overestimation of the trait's reliability [@Hoyle_et_al_2023, pp. 482]. 

<!-- To show the practical consequences, in Figure 18.1, we present the test information for the two models; clearly, the information is positively biased when the multidimensional data are forced into a unidimensional IRT model. Moreover, in this same figure, we show the test response curves (how expected scores change as a function of trait level) for the two models; again, they, clearly, are not overlapping. The estimated implication is that if one applied a unidimensional IRT model, psychometric information is inflated due to the multidimensionality and examinee standard errors, which are the inverse square root of information, are too small. [@Hoyle_et_al_2023, pp. 340-341] -->

<!-- Accounting for measurement error in defining latent variables improves the accuracy of the model’s estimated structural parameters between the latent constructs, which in turn can provide a stronger test of a proposed theoretical model. [@Hoyle_et_al_2023, pp. 482] -->

<!-- Reliability is a necessary but not sufficient condition for validity. Reliability can exist without validity but validity cannot exist without reliability [@Perron_et_al_2015, pp. 2]. -->


## The disconnect between trait measurement and hypothesis testing {#sec-theory-issue2}

<!-- recording: Sven 24.10.04; time: 00:14:30 - 00:21:00 -->
<!-- recording: Sven and Tine 24.10.18; time: 00:08:30 - 00:14:55 -->
<!-- recording: Sven and Tine 24.11.25; time: 00:49:30 - 00:53:30 -->

<!-- 1. The BTL model as a measurement model for estimating latent traits -->
Building on the previous section, it is clear that, despite its limitations, the BTL model is commonly used as the measurement model in CJ assessments. A measurement model specifies how manifest variables contribute to the estimation of latent variables [@Everitt_et_al_2010]. For example, when evaluating text quality, researchers use the BTL model to process the dichotomous outcomes resulting from the pairwise comparisons (the manifest variables) to estimate scores that reflect the underlying quality level of the texts (the latent variable) [@Laming_2004; @Pollitt_2012b; @Whitehouse_2012; @vanDaal_et_al_2016; @Lesterhuis_2018_thesis; @Coertjens_et_al_2017; @Goossens_et_al_2018; @Bouwer_et_al_2023]. 

<!-- 2. Utilizing BTL Scores for further analysis and hypothesis testing -->
Researchers then typically use these estimated BTL scores, or their transformations, to conduct additional analyses or hypothesis tests. For example, these scores have been used to identify 'misfit' judges and stimuli [@Pollitt_2012b; @vanDaal_et_al_2016; @Goossens_et_al_2018], detect biases in judges' ratings [@Pollitt_et_al_2003; @Pollitt_2012b], calculate correlations with other assessment methods [@Goossens_et_al_2018; @Bouwer_et_al_2023], or test hypotheses related to the underlying trait of interest [@Bramley_et_al_2019; @Boonen_et_al_2020; @Bouwer_et_al_2023; @vanDaal_et_al_2017; @Jones_et_al_2019; @Gijsen_et_al_2021].

<!-- 3. But what are the challenges in using BTL scores for further analysis? -->
However, the statistical literature advises caution when using estimated scores for additional analyses and tests. A key consideration is that BTL scores are parameter estimates that inherently carry uncertainty. Ignoring this uncertainty can bias the analysis and reduce the precision of hypothesis tests. Notably, the direction and magnitude of such biases are often unpredictable. Results may be attenuated, exaggerated, or remain unaffected depending on the degree of uncertainty in the scores and the actual effects being tested [@Kline_et_al_2023, pp. 25; @Hoyle_et_al_2023, pp. 137]. Finally, the reduced precision in hypothesis tests diminishes their statistical power, increasing the likelihood of committing type-I or type-II errors [@McElreath_2020]. 

<!-- A type-I error occurs when a *true* null hypothesis is incorrectly rejected, while a type-II error occurs when a *false* null hypothesis is incorrectly accepted [@Everitt_et_al_2010]. -->

In aggregate, researchers' inadequate handling of violations to the assumptions of equal dispersion and zero correlation between stimuli, along with the apparent disconnect between CJ's approach to trait measurement and hypothesis testing, can undermine the reliability of the trait and ultimately compromise its validity [@Perron_et_al_2015, pp. 2]. Consequently, adopting a more systematic and integrated approach to examining what happens when judges compare two stimuli could offer several statistical and measurement benefits, including addressing these issues.

<!-- ## The diverse assessment design features and their role on reliability and validity {#sec-theory-issue3} -->
<!-- Do not include yet!, until you see if you need it when you write about the next section -->
<!-- recording: Sven and Tine 24.11.18; time: 00:47:20 - 00:56:41 -->
<!-- recording: Sven and Tine 24.11.25; time: 00:53:30 - 01:00:00 -->



# An updated theoretical and statistical model for CJ {#sec-theory}

This section presents a theoretical model for CJ that extends Thurstone's theory. The model systematically incorporates all factors involved when judges make pairwise comparisons. Additionally, the section develops the statistical translation of the theoretical model based on assumptions informed by the CJ theory.

<!-- in a stepwise manner -->


## The theoretical model {#sec-theory-theoretical}

<!-- []{style="color:red;"} -->

<!-- general development -->
<!-- recording: Sven 24.10.04; time: 00:21:00 - 00:34:10 -->
<!-- recording: Sven 24.10.04; time: 00:41:00 - 00:46:00 -->
<!-- recording: Sven and Tine 24.10.18; time: 00:00:00 - 00:04:05 -->
<!-- recording: Sven and Tine 24.11.25; time: 01:00:00 - 01:08:40 -->

<!-- comparison mechanism as missing mechanism -->
<!-- recording: Sven 24.10.04; time: 00:09:10 - 00:14:30 -->
<!-- recording: Sven 24.10.04; time: 00:34:10 - 00:41:00 -->


<!-- 6. So, how do we mitigate these risks? -->
<!-- Fortunately, the statistical literature offers a solution to address the abovementioned issues. This solution involves using models that extend traditional approaches to account for the different variability in the stimuli, referred to as over-dispersed models [@McElreath_2020, chap. 12]. -->

<!-- 11. How to solve the violation of the assumption? -->
<!-- Fortunately, the same literature offers solutions for addressing these issues. @Andrich_1978 and @Wainer_et_al_1978 recommend integrating judges' biases into the BTL model. Moreover, the literature advocates for the incorporation of relevant hierarchical structures into the statistical model to account for these dependencies. Together, these additions can result in a model resembling a Multilevel Structural Equation Model (MSEM) [@Hoyle_et_al_2023, chap. 26] combined with a multidimensional or two-parameter logistic Item Response Theory (IRT) model [@Hoyle_et_al_2023, chap. 15], depending on the theoretical and statistical treatment of judges' biases. -->

<!-- Brennen generalizability theory, generalize conclusions over groups. To generalize over judges, you need to estimate them. Akin to Frisch-Wough-Lovel theorem for latent variables?. -->

<!-- 4. So, how do we mitigate these risks? -->
<!-- To mitigate these risks, principles from Structural Equation Modeling (SEM) [@Hoyle_et_al_2023, pp. 138] and IRT [@Fox_2010, chap. 6; @vanderLinden_et_al_2017_I, chap. 24] recommend conducting these analyses and tests within a structural model. A structural model specifies how different manifest or latent variables influence the latent variable of interest [@Everitt_et_al_2010]. This approach allows analyses that can account for both the BTL scores and their uncertainties simultaneously, rather than treating them as separate elements.  -->


<!-- Remember: the "mitigate" paragraph have the purpose of being the design principles that will guide the development of the theoretical and statistical model. -->


<!-- This means that the cancellation of judges' biases may not hold in theory or practice, and that judges' biases may depend on the stimuli, by means of an interaction (but present) or because the judges works as a confounder -->


<!-- no study explicitly proposes that this assumption could also be violated due to the presence of an additional hierarchical (grouping) structure relevant to the texts. One such scenario might arise, for example, when comparing texts produced by university and secondary school students. In this case, university students may consistently (or more precisely) produce higher-quality texts, while secondary school students, who exhibit a broader range of writing abilities, would show greater variability in the quality of their texts. Although this example is somewhat contrived, it effectively illustrates how assuming equal dispersions across texts can overlook meaningful differences in the reliability of text quality across groups or individuals. -->


<!-- The correlation $\rho$, considered above is not the kind of correlation considered by Bock (1958) in studies of preference and choice of objects. The Bock correlation arises from a specific interaction between a subject and an item because the subject has a particular "preference", such as a taste preference, for the object. This preference for the object is then reflected in its comparisons with all other objects. However, it is less likely that subjects would have such particular preferences for some items in a test and, in any case, the assumption is open to empirical checking [@Andrich_1978, pp. 455]. -->


<!-- include subject specific covariates [@Casalicchio_et_al_2015] -->

## From theory to statistics {#sec-theory-statistics}

<!-- recording: Sven 24.10.04; time: 00:46:00 - 01:01:00 -->
<!-- recording: Sven and Tine 24.11.25; time: 00:24:50 - 00:30:35 -->
<!-- recording: Sven and Tine 24.11.25; time: 00:49:30 - 00:53:30 -->

<!-- Using assumptions from IRT [@deAyala_2009, p. 20-21], we are assuming a multidimensional model, that violates the local independece assumption -->

<!-- [@deAyala_2009, p. 291-29] think about indeterminancies in MIRT. to solve metric indeterminancy you commonly fix a mu=0, and s=1, but for rotational indeterminancy, you need to define (maybe) orthogonality between units traits and judges bias -->

<!-- @Pritikin_2020 and @Gray_et_al_2024 bayesian modeling attempts -->


<!-- , as well as when eliminating data through ad hoc univariate procedures -->
<!-- Moreover, excluding data using ad hoc univariate procedures can compound these issues by discarding potentially valuable information, further exacerbating the bias [@Zimmerman_1994; @McElreath_2020] -->


# Discussion {#sec-discuss}

## Findings {#sec-discuss-finding}

<!-- recording: Sven and Tine 24.11.25; time: 00:56:10 - 00:56:10 -->

<!-- "If Thurstone’s Law is to be used in support of comparative judgment for assessment, his theory must be extended to cover the uses and applications that are required for assessment purposes." [@Kelly_et_al_2022, pp. 678] -->


<!-- Pollitt (2012a) argues that assessors in pairwise comparisons need less training, given intuitive comparison and decision making. However, there is a distinct lack of research on the role of training.  -->

<!-- Due to efficiency of scoring, most of research only collects only one text. This assumes that there is significant homogeneity between text of the same individual, compared to the between individual variability. However, this cannot be known ad-hoc.  -->


<!-- (Is this a validity contention?) -->
<!-- Furthermore, it is not inconceivable that the selection of a particular group of judges used to perform the pairwise comparison could result in an incomplete depiction of the dimensions and complexity of the trait. Previous research has highlighted that factors such as age, culture, and education [@Kelly_et_al_2022, pp. 683], as well as individual differences among judges [@Gill_et_al_2013; @vanDaal_et_al_2017; @vanDaal_2020], could influence judgment accuracy, thereby affecting the "shared consensus" of the trait measurement. As @Kelly_et_al_2022 noted [pp. 683], “Would the aggregate view of young, British-Asian men always be the same as the aggregate view of older, black women?” -->

<!-- Such differences may not be detected through analyses of bias and misfit. McMahon and Jones (2015) found differences between students and teachers in what was valued in assessments of understanding of a chemistry experiment. Where students prioritized factual recall, teachers prioritized scientific explanation. Both groups produced highly reliable scales (reliability estimates of .893 for students, and .874 for teachers). Although, in this case, a clear argument as to whose consensus should take priority can be made, it demonstrates the existence of different, but equally reliable, sets of consensuses depending on who is asked. [@Kelly_et_al_2022, pp. 683] -->


<!-- "Another challenge to the intrinsic validity argument is that the aggregate approach and valuing of diversity in expert views (Van Daal et al., 2019) is in tension with the idea of removing misfitting judges (Pollitt, 2012a). If someone is considered an expert, and the validity of comparative judgment rests on the collective expertise of a community of practice, the decision to remove them and discard their judgments can only be justifiable on the assumption that those judgments do not reflect their expertise. ... It is thus unclear whether or not this divergence is welcomed." [@Kelly_et_al_2022, pp. 681] -->

<!-- We advocate for the no removal of judges, but rather identify them and treat the data with the appropriate model, so these 'misfits' do not affect the measurement of the trait -->



## Limitations and further research {#sec-discuss-limitations}


<!-- The field of comparative judgment, while growing, is still relatively small, and at present we do not have compelling answers to fundamental questions such as: how many judges do we need? Who should judge? How many judgments should they make? And crucially, how can we be confident that they are basing their judgments on acceptable criteria? If judges are not using construct-relevant criteria, then the argument that comparative judgments are necessarily valid cannot be upheld. We recognise that there is unlikely to be a single answer to these questions for all the cases in which comparative judgment may be used. However, work to derive some general principles for good practice would be valuable.[@Kelly_et_al_2022, pp. 683] -->

<!-- @Bramley_2008 proposes ranking instead of comparisons -->


# Conclusion {#sec-conclusion}



{{< pagebreak >}}

# Declarations {.appendix .unnumbered}

**Funding:** The project was founded through the Research Fund of the University of Antwerp (BOF).

**Financial interests:** The authors have no relevant financial interest to disclose.

**Non-financial interests:** The authors have no relevant non-financial interest to disclose.

**Ethics approval:** The University of Antwerp Research Ethics Committee has confirmed that no ethical approval is required.

**Consent to participate:** Not applicable

**Consent for publication:** All authors have read and agreed to the published version of the manuscript.

**Availability of data and materials:** No data was utilized in this study.

**Code availability:** All the code utilized in this research is available in the digital document located at: [https://jriveraespejo.github.io/paper2_manuscript/](https://jriveraespejo.github.io/paper2_manuscript/).  

**AI-assisted technologies in the writing process:** The authors used ChatGPT, an AI language model, during the preparation of this work. They occasionally employed the tool to refine phrasing and optimize wording, ensuring appropriate language use and enhancing the manuscript's clarity and coherence. The authors take full responsibility for the final content of the publication.

**CRediT authorship contribution statement:** *Conceptualization:* S.G., S.DM., T.vD., and J.M.R.E; *Methodology:* S.DM., T.vD., and J.M.R.E; *Software:* J.M.R.E.; *Validation:* J.M.R.E.; *Formal Analysis:* J.M.R.E.; *Investigation:* J.M.R.E; *Resources:* S.G., S.DM., and T.vD.; *Data curation:* J.M.R.E.; *Writing - original draft:* J.M.R.E.; *Writing - review and editing:* S.G., S.DM., and T.vD.; *Visualization:* J.M.R.E.; *Supervision:* S.G. and S.DM.; *Project administration:* S.G. and S.DM.; *Funding acquisition:* S.G. and S.DM.

<!-- **Acknowledgements:** -->



{{< pagebreak >}}

# References {.unnumbered}

:::{#refs}

:::