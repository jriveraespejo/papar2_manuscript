---
title: "Causes and effects in Dichotomous Comparative Judgments: an information-theoretical system of plausible mechanism"
author:
  - name: 
      given: Jose Manuel
      family: Rivera Espejo
    orcid: 0000-0002-3088-2783
    url: https://www.uantwerpen.be/en/staff/jose-manuel-rivera-espejo_23166/
    email: JoseManuel.RiveraEspejo@uantwerpen.be
    corresponding: true
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Tine
      family: van Daal
      non-dropping-particle: van
    orcid: https://orcid.org/0000-0001-9398-9775
    url: https://www.uantwerpen.be/en/staff/tine-vandaal/
    email: tine.vandaal@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Sven
      family: De Maeyer
      non-dropping-particle: De
    orcid: 0000-0003-2888-1631
    url: https://www.uantwerpen.be/en/staff/sven-demaeyer/
    email: sven.demaeyer@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Steven
      family: Gillis
    orcid: 
    url: https://www.uantwerpen.be/nl/personeel/steven-gillis/
    email: steven.gillis@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Linguistics
        group: Centre for computational linguistics, psycholinguistics, and sociolinguistics (CLiPS)
funding: 
  statement: "The project was founded through the Research Fund of the University of Antwerp (BOF)."
keywords:
  - causal inference
  - probability
  - Thurstone
  - comparative judgement
  - directed acyclic graph
  - structural causal models
  - statistical modeling
abstract: |
  (to do)
key-points:
  - (to do)
date: last-modified
bibliography: references.bib
notebook-links: global
lightbox: true
citation: true
  # type: article-journal
  # container-title: "Psychometrika"
  # doi: ""
  # # url: https://example.com/summarizing-output
execute: 
  cache: true
  # eval: true
  echo: false
  # output: true
  # include: true
  warning: false
  error: false
  message: false
---

# Introduction {#sec-introduction}

<!-- recording -->
<!-- Part 1; 00:00:00 - 00:09:10 -->
<!-- Part 2; 00:14:30 - 00:21:00 -->

<!-- 1. Where are we now with DCJ? -->

<!-- Notes: -->
<!-- - In the past decade there has been a growing number of studies that document and analyze the merits of DCJ as an assessment tool. Define a bit DCJ. -->
<!-- - It has been a growing field, where people has tested some important aspects of the method. In general, is an iterative process. People are iterating on particular aspects of the method, such as: -->
<!-- a. issues within the method as such, critical analyze some aspect of the method -->
<!-- b. practical use of the method, generate scores with the BTL, and use this scores with some hypotheses, or descriptions. -->

Over the past decade, numerous studies have documented the effectiveness of the *comparative judgment* (CJ) method [@Thurstone_1927; @Pollitt_2012a] for assessing competencies and traits. These studies have evaluated CJ from two main perspectives: its ability to produce reliable and valid trait scores, and its practical applicability. In terms of reliability and validity, research has shown that CJ can generate precise and consistent scores that accurately represent the traits being measured. Notable contributions in this research area include studies by @Pollitt_2012b, @Whitehouse_2012, @vanDaal_et_al_2016, @Lesterhuis_2018, @vanDaal_et_al_2019, @Bramley_et_al_2019, @Verhavert_et_al_2019, @Crompvoets_et_al_2022, and @Bouwer_et_al_2023. Regarding practical applicability, several studies have highlighted the method's versatility in both educational and non-educational contexts, presenting it as an efficient and effective alternative for measurement and evaluation. Key examples in this research area include the works of @Jones_2015, @Bartholomew_et_al_2018, @Jones_et_al_2019, @Marshall_et_al_2020, @Bartholomew_et_al_2020, and @Boonen_et_al_2020.

<!-- 2. But what are the problems of DCJ as a method? -->

<!-- Notes:  -->
<!-- - But what it is the problem with this way of using DCJ (just one paragraph) -->
<!-- a. measurement model is based on case V -->
<!--   a.1 It does not consider the effects of judges (different perceptions per stimuli) -->
<!--   a.2 It does not consider that stimuli may be nested in individuals -->
<!-- b. in all literature there is a disconnection between measurement model and the structural part (common practical use) -->
<!-- c. what about the experimental design?, what are the implications of certain comparison algorithms? -->

Nevertheless, despite the growing number of CJ studies, the research approaches employed in the literature have been unsystematic and non-integrated, leading to the oversight of several critical issues related to the method. These issues include concerns about the measurement model responsible for generating the CJ scores, the structural component used for further data analysis and hypothesis testing, and challenges related to the design of CJ experiments. For instance, a notable concern regarding the measurement model is the prevalent reliance on the assumptions of Case 5 from Thurstone's law of comparative judgment [-@Thurstone_1927]. Although Case 5 was originally articulated to produce a "rough measurement" or "rather coarse scaling" of traits [@Thurstone_1927, p. 268-269], its assumptions have become predominant in the literature due to its implementation through the Bradley-Terry-Luce (BTL) model [@Bradley_et_al_1952; @Luce_1959]. This leaves issues such as the presence of judge' biases hinted by @Bramley_2008 and @Kelly_et_al_2022, and evidenced by @Pollitt_et_al_2003, @Arlett_2003, and @Guthrie_2003

<!-- a practice that may not be justifiable. -->


<!-- One issue related to the structural component, is the disconnection existing in the literature between the measurement model and the structural component used for hypothesis testing,  -->


<!-- and issues related to the method's experimental design. These issues can affect the method's reliability, validity, and practical applicability. -->



<!-- 3. What is the solution? -->

<!-- Notes:  -->
<!-- - There is some gain on considering a more integrated overarching systematic way of looking on what happen in DCJ when people compare two stimuli -->
<!-- - This paper tries to systematically integrate all aspects at play in a single scientific model, we will build the model in a stepwise manner using -->
<!-- - we will build a scientific model (more theoretical model) that integrates different aspects of the method that are at play when people use DCJ. -->



# Theory {#sec-theory}
<!-- 21:00 recording -->



## Let's talk about Thurstone co. {#sec-theory-thurstone}

<!-- Notes: -->
<!-- - But what it is the problem with this way of using DCJ (just one paragraph) -->
<!-- a. measurement model is based on case V -->
<!--   a.1 It does not consider the effects of judges (different perceptions per stimuli) -->
<!--   a.2 It does not consider that stimuli may be nested in individuals -->
<!-- b. in all literature there is a disconnection between measurement model and the structural part (common practical use) -->
<!-- c. what about the experimental design?, what are the implications of certain comparison algorithms? -->


<!-- Use the SEM description of measurement model and structural model. -->

<!-- The issues fall into three categories: those related to the measurement model, the structural model, and the experimental design. -->

<!-- Reliability is a necessary but not sufficient condition for validity. Reliability can exist without validity but validity cannot exist without reliability [@Perron_et_al_2015]. -->

<!-- Measurement model -->

<!-- @Thurstone_1927 justify the use of case V, on multiple assumptions, but the most important for our purpose are three:  -->

<!-- (a) related to Case 3, it assumes the correlation between stimuli is zero, this translates into the cancellation of judges effects by mean of opposing and equally weigthed 'mood' and 'simultaneous contrasts' effects. This is demonstrated using the additivive nature of the logit scale, which helps to cancel 'bias' judges effects. -->
<!-- This is the case of confounding, and @Andrich_1978 already showed that if judges effects are parametrized, it gets eliminated experimentally in the paired comparison method.  -->

<!-- possibility of bias is hinted in @Bramley_2008, and later mentioned again in @Kelly_et_al_2022, and some evidence is found in @Pollitt_et_al_2003, @Arlett_2003, @Guthrie_2003 and @Gijsen_et_al_2021 -->


<!-- (b) related to Case 3, it assumes the correlation between stimuli is zero, this also translates into the idea that stimuli are the main focus of estimation and analysis, but what happens when the focus of analysis is the individuals that generated those stimuli. There is an amount of correlation that it is not accounted for.  -->
<!-- This is the case of clustering or measurement error, and overlooking issues such as clustering and measurement error can lead to biased and less precise parameter estimates [@McElreath_2020], ultimately diminishing the statistical power of models and increasing the likelihood of committing type I or type II errors when addressing research inquiries.  -->

<!-- This has been considered in the literature as a (see https://doi.org/10.1177/1471082X15571817) or a multilevel BTL model (find sources) -->

<!-- (c) related to case 5, discriminal dispersions of the stimuli are equal, but it is not hard to imagine that certain individuals can produce good quality texts with more precision than other individuals.  -->

<!-- This ties up with the next subject -->



<!-- Structural model -->

<!-- Regarding this, it is apparent that in all literature there is a disconnection between measurement model and the structural part, where hypothesis and summaries are performed. -->

<!-- The current literature estimates the CJ scores to later used them into other analysis, such as investigating bias and misfits among judges, or for the comparison of groups of individuals. This ignores measurement error. -->



<!-- Experimental design -->

<!-- In this regard, although the literature has tested multiple features of the experimental design of the method, such as the comparison algorithm and the number of comparisons, it is not clear where this procedures fit in the whole process of comparison, and what are their implications for the outcome of the method. -->

<!-- present the evidence of bias effects of adaptive algorithms, the not clear view of how many comparisons per (sub)units are required. Adhoc rules of thumb. -->


## A scientific model for the CJ {#sec-theory-scientific}

## From theory to statistical model {#sec-theory-statmodel}


# Discussion {#sec-discuss}

## Findings {#sec-discuss-finding}


## Limitations and further research {#sec-discuss-limitations}



# Conclusion {#sec-conclusion}



{{< pagebreak >}}

# Declarations {.appendix .unnumbered}

**Funding:** The project was founded through the Research Fund of the University of Antwerp (BOF).

**Financial interests:** The authors have no relevant financial interest to disclose.

**Non-financial interests:** Author XX serve on advisory broad of Company Y but receives no compensation this role.

**Ethics approval:** The University of Antwerp Research Ethics Committee has confirmed that no ethical approval is required.

**Consent to participate:** Not applicable

**Consent for publication:** All authors have read and agreed to the published version of the manuscript.

**Availability of data and materials:** No data was utilized in this study.

**Code availability:** All the code utilized in this research is available in the digital document located at: [https://jriveraespejo.github.io/paper2_manuscript/](https://jriveraespejo.github.io/paper2_manuscript/).  

**Authors' contributions:** *Conceptualization:* S.G., S.DM., T.vD., and J.M.R.E; *Methodology:* S.DM., T.vD., and J.M.R.E; *Software:* J.M.R.E.; *Validation:* J.M.R.E.; *Formal Analysis:* J.M.R.E.; *Investigation:* J.M.R.E; *Resources:* S.G., S.DM., and T.vD.; *Data curation:* J.M.R.E.; *Writing - original draft:* J.M.R.E.; *Writing - review & editing:* S.G., S.DM., and T.vD.; *Visualization:* J.M.R.E.; *Supervision:* S.G. and S.DM.; *Project administration:* S.G. and S.DM.; *Funding acquisition:* S.G. and S.DM.

<!-- **Acknowledgements:** -->



{{< pagebreak >}}

# Appendix {#sec-appendix}

{{< pagebreak >}}

## References {.unnumbered}

:::{#refs}

:::