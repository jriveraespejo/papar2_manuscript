---
title: "Let's talk about Thurstone & Co.: An information-theoretical model for comparative judgments, and its statistical translation"
author:
  - name: 
      given: Jose Manuel
      family: Rivera Espejo
    orcid: 0000-0002-3088-2783
    url: https://www.uantwerpen.be/en/staff/jose-manuel-rivera-espejo_23166/
    email: JoseManuel.RiveraEspejo@uantwerpen.be
    corresponding: true
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Tine
      family: van Daal
      non-dropping-particle: van
    orcid: https://orcid.org/0000-0001-9398-9775
    url: https://www.uantwerpen.be/en/staff/tine-vandaal/
    email: tine.vandaal@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Sven
      family: De Maeyer
      non-dropping-particle: De
    orcid: 0000-0003-2888-1631
    url: https://www.uantwerpen.be/en/staff/sven-demaeyer/
    email: sven.demaeyer@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Training and education sciences
        group: Edubron
  - name: 
      given: Steven
      family: Gillis
    orcid: 
    url: https://www.uantwerpen.be/nl/personeel/steven-gillis/
    email: steven.gillis@uantwerpen.be
    corresponding: false
    affiliation:
      - name: University of Antwerp
        department: Linguistics
        group: Centre for computational linguistics, psycholinguistics, and sociolinguistics (CLiPS)
funding: 
  statement: "The project was founded through the Research Fund of the University of Antwerp (BOF)."
keywords:
  - causal inference
  - probability
  - Thurstone
  - comparative judgement
  - directed acyclic graph
  - structural causal models
  - statistical modeling
abstract: |
  (to do)
key-points:
  - (to do)
date: last-modified
bibliography: references.bib
notebook-links: global
lightbox: true
citation: true
  # type: article-journal
  # container-title: "Psychometrika"
  # doi: ""
  # # url: https://example.com/summarizing-output
execute: 
  cache: true
  # eval: true
  echo: false
  # output: true
  # include: true
  warning: false
  error: false
  message: false
---

# Introduction {#sec-introduction}

<!-- recording 1 -->
<!-- Part 1; 00:00:00 - 00:09:10 -->

<!-- recording 2 -->
<!-- Part 2; 00:00:00 - 00:17:30 -->

<!-- 1. Where are we now with DCJ? -->
Over the past decade, numerous studies have documented the effectiveness of the *comparative judgment* (CJ) method [@Thurstone_1927] for assessing competencies and traits. These studies have evaluated CJ from three main perspectives: its ability to produce reliable and valid trait scores, its practical applicability, and its time efficiency. Research on reliability and validity shows that CJ can generate precise and consistent scores [@Pollitt_2012a; @Pollitt_2012b; @Coertjens_et_al_2017; @Goossens_et_al_2018; @Verhavert_et_al_2019; @Crompvoets_et_al_2022; @Bouwer_et_al_2023] that accurately represent the traits being measured [@Whitehouse_2012; @vanDaal_et_al_2016; @Lesterhuis_2018; @Bouwer_et_al_2023]. Research on practical applicability highlights CJ's versatility across both educational and non-educational contexts, presenting it as an efficient and effective alternative for measurement and evaluation [@Pollitt_2004; @Jones_2015; @Bartholomew_et_al_2018; @Jones_et_al_2019; @Marshall_et_al_2020; @Bartholomew_et_al_2020; @Boonen_et_al_2020]. Lastly, research on time efficiency suggests that CJ can offer at least equal, if not significant, time savings when evaluating stimuli compared to traditional marking methods [@Pollitt_2012a; @Pollitt_2012b; @Coertjens_et_al_2017; @Goossens_et_al_2018].

<!-- 2. But what are the problems of DCJ as a method? -->
Nevertheless, despite the growing number of CJ studies, the unsystematic and fragmented research approaches employed in the literature have overlooked several critical issues concerning the method. These issues fall into three main categories: the disconnect between CJ's structural and measurement model, the over-reliance on Thurstone's Case 5 [-@Thurstone_1927] in its measurement model, and the role of the comparison algorithms and the number of comparisons on the method's reliability and validity. In the following sections, each issue will be discussed in detail, followed by the introduction of a theoretical model and its translation into a statistical model that addresses all three concerns simultaneously.


# Overlooked issues in CJ literature {#sec-theory-issues}

## The disconnect between structural and measurement model

<!-- 1. The BTL model is CJ's measurement model -->
In the CJ literature, the Bradley-Terry-Luce (BTL) model [@Bradley_et_al_1952; @Luce_1959] serves as the measurement model for CJ because it specifies how latent variables are estimated from observed ones [@Hoyle_et_al_2023; @Kline_et_al_2023]. In a CJ study, multiple judges conduct several rounds of pairwise comparisons to evaluate the relative manifestation of a trait between a pair of stimuli. This evaluation results in a dichotomous outcome that indicates which stimulus is perceived to exhibit a higher degree of the trait. The BTL model then uses these observed outcomes to estimate scores that represent the latent trait of interest [@Pollitt_2012a; @Pollitt_2012b; @Whitehouse_2012; @Jones_2015; @vanDaal_et_al_2016; @Lesterhuis_2018; @Boonen_et_al_2020; @Bouwer_et_al_2023].

<!-- 2. But there is no structural model, only post estimation analyses -->
Moreover, in the CJ literature, it is common practice for BTL-generated scores, or their transformations, to undergo additional data analysis and hypothesis testing. Researchers have utilized these scores in independent analyses to identify 'misfit' judges and stimuli [@Pollitt_2012b; @vanDaal_et_al_2017; @Goossens_et_al_2018], detect biases in judges' ratings [@Pollitt_et_al_2003; @Pollitt_2012b], calculate correlations with other scoring methods [@Goossens_et_al_2018; @Bouwer_et_al_2023], or test hypotheses related to the trait of interest [@Bramley_et_al_2019; @Boonen_et_al_2020; @Bouwer_et_al_2023; @vanDaal_et_al_2017; @Jones_et_al_2019; @Gijsen_et_al_2021]. 

<!-- 3. But what is the problem with this? -->
<!-- However, the statistical literature cautions when using estimated scores for further independent data analysis and hypothesis testing. Since BTL-generated scores are parameter estimates that carry inherent uncertainty, separating the subsequent analyses from this uncertainty may lead to an artificial inflation of the precision and power of the analyses and tests. Consequently, this practice could increase the risk of committing a type I error, wherein a null hypothesis is incorrectly rejected [@McElreath_2020]. An appropriate approach to address this issue, consistent with the principles of Structural Equation Modeling (SEM) [@Hoyle_et_al_2023; @Kline_et_al_2023], is that these analyses should be conducted at the structural level rather than as independent analyses. Consequently, an approach that integrates the structural and measurement model should be followed. -->

<!-- However, there is often a lack of integration between the structural model, which captures the relationships among the variables, and the measurement model, which generates the CJ scores. This disconnect can lead to inaccurate inferences about the traits being measured. -->


## Case 5 and the measurement model

<!-- Measurement model -->

<!-- a. measurement model is based on case V -->
<!--   a.1 It does not consider the effects of judges (different perceptions per stimuli) -->
<!--   a.2 It does not consider that stimuli may be nested in individuals -->



<!-- @Thurstone_1927 justify the use of case V, on multiple assumptions, but the most important for our purpose are three:  -->

<!-- (a) related to Case 3, it assumes the correlation between stimuli is zero, this translates into the cancellation of judges effects by mean of opposing and equally weigthed 'mood' and 'simultaneous contrasts' effects. This is demonstrated using the additivive nature of the logit scale, which helps to cancel 'bias' judges effects. -->
<!-- This is the case of confounding, and @Andrich_1978; @Wainer_et_al_1978 already showed that if judges effects are parametrized, it gets eliminated experimentally in the paired comparison method.  -->

<!-- possibility of bias is hinted in @Bramley_2008, and later mentioned again in @Kelly_et_al_2022, and some evidence is found in @Pollitt_et_al_2003, @Arlett_2003, @Guthrie_2003, @vanDaal_et_al_2017 and @Gijsen_et_al_2021 -->

<!-- @Pollitt_et_al_2003 In Thurstone’s method each judge’s standard cancels out when a comparison is made, and any misfit between data and model must indicate a difference of some other kind. This is studied by analysing the residuals that remain after the model is fitted to the data. Whenever one script is judged to have a higher value than a competing one it is expected to ‘win’ the comparison: if it does the residual will be small, but if it loses then the residual for that judge’s comparison of those two scripts will be large. Any patterns of large residuals will indicate some sort of bias – in this context bias simply means a source of systematic, rather than random, variability. indicates that one judge – Judge 5 – is significantly biased in some way. (Consider the problem of using adhoc methods) -->


<!-- Reliability is a necessary but not sufficient condition for validity. Reliability can exist without validity but validity cannot exist without reliability [@Perron_et_al_2015]. -->


<!-- (b) related to Case 3, it assumes the correlation between stimuli is zero, this also translates into the idea that stimuli are the main focus of estimation and analysis, but what happens when the focus of analysis is the individuals that generated those stimuli. There is an amount of correlation that it is not accounted for.  -->
<!-- This is the case of clustering or measurement error, and overlooking issues such as clustering and measurement error can lead to biased and less precise parameter estimates [@McElreath_2020], ultimately diminishing the statistical power of models and increasing the likelihood of committing type I or type II errors when addressing research inquiries.  -->

<!-- This has been considered in the literature as a (see https://doi.org/10.1177/1471082X15571817) or a multilevel BTL model (find sources) -->

<!-- (c) related to case 5, discriminal dispersions of the stimuli are equal, but it is not hard to imagine that certain individuals can produce good quality texts with more precision than other individuals.  -->

<!-- This ties up with the next subject -->


<!-- Moreover, a notable concern in the second category is the measurement model's reliance on the assumptions of Case 5 from Thurstone's law of comparative judgment [-@Thurstone_1927]. As noted, it is common practice in the literature to use the BTL model as a means of generating CJ scores, serving as the measurement model. -->

<!-- Since the study by @Bramley_2008, it has been clear that the BTL model represents a statistical articulation of Case 5. -->

<!-- Although Case 5 was originally articulated to produce a "rather coarse scaling" of traits [@Thurstone_1927, p. 269], its assumptions have become predominant in the literature due to its implementation through the Bradley-Terry-Luce (BTL) model [@Bradley_et_al_1952; @Luce_1959]. This leaves issues such as the presence of judge' biases hinted by @Bramley_2008 and @Kelly_et_al_2022, and evidenced by @Pollitt_et_al_2003, @Arlett_2003, and @Guthrie_2003 -->


<!-- All studies use SSR as a measure of reliability -->


## The efficiency and reliability of comparison algorithms

<!-- Experimental design -->

<!-- In this regard, although the literature has tested multiple features of the experimental design of the method, such as the comparison algorithm and the number of comparisons, it is not clear where this procedures fit in the whole process of comparison, and what are their implications for the outcome of the method. -->

<!-- @Bramley_et_al_2019 and @Bramley_2015 says adaptivity inflates reliability. present the evidence of bias effects of adaptive algorithms, the not clear view of how many comparisons per (sub)units are required. Adhoc rules of thumb. -->

<!-- @Gray_et_al_2024 entropy-driven active learning pair-selection algorithm to compare stimuli, vs random and no repeating pairs -->

<!-- @DeVrindt_et_al_2024 cold start problem to optimize time efficiency. -->

<!-- @Mikhailiuk_et_al_2021 proposed a fully Bayesian active sampling strategy for pairwise comparisons via Approximate Message Passing and Information Gain Maximization. ASAP computes the full posterior distribution, which is crucial to achieving accurate EIG estimates, and thus the accuracy of active sampling. We recommend ASAP, as it offered the highest accuracy of inferred scores compared to existing methods in experiments with real and synthetic data. -->

<!-- @Verhavert_et_al_2022 The current article proposes a new adaptive selection algorithm using a previously calibrated reference set. Using a reference set should eliminate the reliability inflation. It is proven that this adaptive selection algorithm is more efficient without reducing the accuracy of the results and without increasing the standard deviation of the assessment results. Akin to BIB designs and more specifically to BTIB designs  -->


<!-- and issues related to the method's experimental design. These issues can affect the method's reliability, validity, and practical applicability. -->


<!-- @Coertjens_et_al_2017 assess reliability and time investment. We conclude that pairwise comparison requires a similar time investment from evaluators as the criterion list method. We want to note that the outcomes in this study depend heavily on the design. First, the time taken for an assessment is strongly determined by the length of the criteria list used and the length of the texts (Breland, 1983). Not adaptive -->


<!-- Although the process of pairwise comparisons is quite easy and fast for raters to apply (@Laming_2004; @Pollitt_2004), even without training, the increase in time investment that is needed for CJ decreases its feasibility in educational practice. -->


<!-- Messick_1989 talks about efficiency and feasibility for application  -->

<!-- @Bramley_2008 proposes ranking instead of comparisons -->


<!-- talk about the number of comparisons @Verhavert_et_al_2019, @Crompvoets_et_al_2022, @Pollitt_2004 -->


<!-- @Pollitt_2012b concludes that speed is not related to the quality of assessments in pairwise comparisons -->



## What is the solution

<!-- 3. What is the solution? -->

<!-- Notes:  -->
<!-- - There is some gain on considering a more integrated overarching systematic way of looking on what happen in DCJ when people compare two stimuli -->
<!-- - This paper tries to systematically integrate all aspects at play in a single scientific model, we will build the model in a stepwise manner using -->
<!-- - we will build a scientific model (more theoretical model) that integrates different aspects of the method that are at play when people use DCJ. -->



# Theory {#sec-theory}
<!-- recording 1 -->
<!-- Part 2; 00:14:30 - 00:21:00 -->



## A theoretical model for the CJ {#sec-theory-scientific}

## From theory to statistical model {#sec-theory-statmodel}

<!-- @Pritikin_2020 and @Gray_et_al_2024 bayesian modeling attempts -->

# Discussion {#sec-discuss}

## Findings {#sec-discuss-finding}

<!-- Pollitt (2012a) argues that assessors in pairwise comparisons need less training, given intuitive comparison and decision making. However, there is a distinct lack of research on the role of training.  -->

## Limitations and further research {#sec-discuss-limitations}



# Conclusion {#sec-conclusion}



{{< pagebreak >}}

# Declarations {.appendix .unnumbered}

**Funding:** The project was founded through the Research Fund of the University of Antwerp (BOF).

**Financial interests:** The authors have no relevant financial interest to disclose.

**Non-financial interests:** Author XX serve on advisory broad of Company Y but receives no compensation this role.

**Ethics approval:** The University of Antwerp Research Ethics Committee has confirmed that no ethical approval is required.

**Consent to participate:** Not applicable

**Consent for publication:** All authors have read and agreed to the published version of the manuscript.

**Availability of data and materials:** No data was utilized in this study.

**Code availability:** All the code utilized in this research is available in the digital document located at: [https://jriveraespejo.github.io/paper2_manuscript/](https://jriveraespejo.github.io/paper2_manuscript/).  

**Authors' contributions:** *Conceptualization:* S.G., S.DM., T.vD., and J.M.R.E; *Methodology:* S.DM., T.vD., and J.M.R.E; *Software:* J.M.R.E.; *Validation:* J.M.R.E.; *Formal Analysis:* J.M.R.E.; *Investigation:* J.M.R.E; *Resources:* S.G., S.DM., and T.vD.; *Data curation:* J.M.R.E.; *Writing - original draft:* J.M.R.E.; *Writing - review & editing:* S.G., S.DM., and T.vD.; *Visualization:* J.M.R.E.; *Supervision:* S.G. and S.DM.; *Project administration:* S.G. and S.DM.; *Funding acquisition:* S.G. and S.DM.

<!-- **Acknowledgements:** -->



{{< pagebreak >}}

# Appendix {#sec-appendix}

{{< pagebreak >}}

## References {.unnumbered}

:::{#refs}

:::